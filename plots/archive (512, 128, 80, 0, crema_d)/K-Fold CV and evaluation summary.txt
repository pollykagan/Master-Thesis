# channels=[80, 60, 40, 20]; mlp_units=[800, 80]; pooling_indices=[0, 1, 3]; pooling_size=3; kernel_size=3; dropout=0.18; regularization_coefficient=0; batch_size=48; learning_rate=0.005
# channels=[80, 60, 40, 20]; mlp_units=[800, 80]; pooling_indices=[0, 1, 3]; pooling_size=3; kernel_size=3; dropout=0.21; regularization_coefficient=0; batch_size=48; learning_rate=0.001
# channels=[80, 60, 40, 20]; mlp_units=[800, 80]; pooling_indices=[0, 1, 3]; pooling_size=3; kernel_size=3; dropout=0.24; regularization_coefficient=0; batch_size=32; learning_rate=0.001
# channels=[80, 60, 40, 20]; mlp_units=[800, 80]; pooling_indices=[0, 1, 3]; pooling_size=3; kernel_size=3; dropout=0.3; regularization_coefficient=0; batch_size=16; learning_rate=0.001
# channels=[80, 60, 40, 20]; mlp_units=[800, 80]; pooling_indices=[0, 1, 3]; pooling_size=3; kernel_size=3; dropout=0.27; regularization_coefficient=0; batch_size=48; learning_rate=0.001
# channels=[80, 60, 40, 20]; mlp_units=[800, 80]; pooling_indices=[0, 1, 3]; pooling_size=3; kernel_size=3; dropout=0.18; regularization_coefficient=1e-06; batch_size=32; learning_rate=0.001
# channels=[80, 60, 40, 20]; mlp_units=[800, 80]; pooling_indices=[0, 1, 3]; pooling_size=3; kernel_size=3; dropout=0.24; regularization_coefficient=0; batch_size=48; learning_rate=0.005
# channels=[80, 60, 40, 20]; mlp_units=[800, 80]; pooling_indices=[0, 1, 3]; pooling_size=3; kernel_size=3; dropout=0.21; regularization_coefficient=1e-06; batch_size=48; learning_rate=0.001
# SUMMARY for this architecture: pooling_size=3; kernel_size=3; dropout=0.21; regularization_coefficient=0; batch_size=48; learning_rate=0.001

# channels=[20, 40, 30, 10]; mlp_units=[400, 100, 13]; pooling_indices=[0, 1]; pooling_size=3; kernel_size=4; dropout=0.3; regularization_coefficient=1e-05; batch_size=48; learning_rate=0.005
# channels=[20, 40, 30, 10]; mlp_units=[400, 100, 13]; pooling_indices=[0, 1]; pooling_size=3; kernel_size=4; dropout=0.24; regularization_coefficient=1e-05; batch_size=32; learning_rate=0.001
# channels=[20, 40, 30, 10]; mlp_units=[400, 100, 13]; pooling_indices=[0, 1]; pooling_size=3; kernel_size=4; dropout=0.21; regularization_coefficient=1e-05; batch_size=48; learning_rate=0.005
# channels=[20, 40, 30, 10]; mlp_units=[400, 100, 13]; pooling_indices=[0, 1]; pooling_size=3; kernel_size=4; dropout=0.21; regularization_coefficient=0; batch_size=32; learning_rate=0.001
# channels=[20, 40, 30, 10]; mlp_units=[400, 100, 13]; pooling_indices=[0, 1]; pooling_size=3; kernel_size=4; dropout=0.21; regularization_coefficient=1e-06; batch_size=16; learning_rate=0.005
# channels=[20, 40, 30, 10]; mlp_units=[400, 100, 13]; pooling_indices=[0, 1]; pooling_size=3; kernel_size=4; dropout=0.3; regularization_coefficient=0; batch_size=48; learning_rate=0.005
# channels=[20, 40, 30, 10]; mlp_units=[400, 100, 13]; pooling_indices=[0, 1]; pooling_size=3; kernel_size=4; dropout=0.27; regularization_coefficient=1e-06; batch_size=16; learning_rate=0.001
# channels=[20, 40, 30, 10]; mlp_units=[400, 100, 13]; pooling_indices=[0, 1]; pooling_size=3; kernel_size=4; dropout=0.15; regularization_coefficient=0; batch_size=48; learning_rate=0.005
# SUMMARY for this architecture: pooling_size=3; kernel_size=4; dropout=0.24; regularization_coefficient=1e-06; batch_size=48; learning_rate=0.005

# channels=[50, 30, 20, 10]; mlp_units=[300, 100, 20]; pooling_indices=[0, 1, 3]; pooling_size=3; kernel_size=3; dropout=0.15; regularization_coefficient=1e-05; batch_size=48; learning_rate=0.005
# channels=[50, 30, 20, 10]; mlp_units=[300, 100, 20]; pooling_indices=[0, 1, 3]; pooling_size=3; kernel_size=3; dropout=0.21; regularization_coefficient=1e-05; batch_size=16; learning_rate=0.001

# EVALUTION for [80, 60, 40, 20]
# Training Loss = 0.02306610904633999. Training Accuracy = 0.9900990128517151
# Validation Loss = 0.1974678486585617. Validation Accuracy = 0.9406779408454895
# Test Loss = 0.27637091279029846. Test Accuracy = 0.9152542352676392

# EVALUTION for [20, 40, 30, 10]
# Training Loss = 0.013703999109566212. Training Accuracy = 0.9964639544487
# Validation Loss = 0.36717984080314636. Validation Accuracy = 0.9385592937469482
# Test Loss = 0.38408997654914856. Test Accuracy = 0.9385592937469482